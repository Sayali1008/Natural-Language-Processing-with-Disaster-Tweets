{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17777,"databundleVersionId":869809,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-25T16:59:08.356244Z","iopub.execute_input":"2024-08-25T16:59:08.356768Z","iopub.status.idle":"2024-08-25T17:01:52.299032Z","shell.execute_reply.started":"2024-08-25T16:59:08.356728Z","shell.execute_reply":"2024-08-25T17:01:52.297354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport re\nimport wordcloud\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-25T17:01:52.301952Z","iopub.execute_input":"2024-08-25T17:01:52.303117Z","iopub.status.idle":"2024-08-25T17:01:53.615393Z","shell.execute_reply.started":"2024-08-25T17:01:52.303054Z","shell.execute_reply":"2024-08-25T17:01:53.614132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Task","metadata":{}},{"cell_type":"markdown","source":"The dataset contains a bunch of tweets and keywords and locations associated with those tweets. The task is to predict whether these tweets are about disasters or not. Tweets contain a variety of word associations including hastags, emoticons, spelling mistakes and much more. The data needs to be drilled down, cleaned, and preprocessed before designing a machine learning model to classify whether the tweet is about a real disaster or not.","metadata":{}},{"cell_type":"markdown","source":"# The Data","metadata":{}},{"cell_type":"markdown","source":"The training and testing data files are structured labelled data sets that contain the following columns:\n1. id - identifier for every tweet\n2. keyword - a keyword from the tweet (it could be NaN)\n3. location - place where tweet was sent from (it could be NaN)\n4. text -  text in the tweet\n5. target - 0/1\n\nThe data preprocessing steps will involve - lowercasing, removing hastags, URL, links, punctuations, handling missing data, stop words removal, lemmatization. Once this is done, divide the train dataset into feature and label.\n\nBefore preprocessing the data, check how the data is distributed between the two target values.\nAfter preprocessing the data, create a word cloud to understand how data is.","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ndf_test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:53.616798Z","iopub.execute_input":"2024-08-25T17:01:53.617249Z","iopub.status.idle":"2024-08-25T17:01:53.693956Z","shell.execute_reply.started":"2024-08-25T17:01:53.617218Z","shell.execute_reply":"2024-08-25T17:01:53.692756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:53.696591Z","iopub.execute_input":"2024-08-25T17:01:53.696974Z","iopub.status.idle":"2024-08-25T17:01:53.730505Z","shell.execute_reply.started":"2024-08-25T17:01:53.696945Z","shell.execute_reply":"2024-08-25T17:01:53.728986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:53.732276Z","iopub.execute_input":"2024-08-25T17:01:53.732768Z","iopub.status.idle":"2024-08-25T17:01:53.755035Z","shell.execute_reply.started":"2024-08-25T17:01:53.732725Z","shell.execute_reply":"2024-08-25T17:01:53.753670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Handle missing values\ndf_train['keyword'] = df_train['keyword'].fillna('')\ndf_train['location'] = df_train['location'].fillna('')\ndf_train['text'] = df_train['text'].fillna('')\n\ndf_test['keyword'] = df_test['keyword'].fillna('')\ndf_test['location'] = df_test['location'].fillna('')\ndf_test['text'] = df_test['text'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:53.756598Z","iopub.execute_input":"2024-08-25T17:01:53.757577Z","iopub.status.idle":"2024-08-25T17:01:53.775858Z","shell.execute_reply.started":"2024-08-25T17:01:53.757542Z","shell.execute_reply":"2024-08-25T17:01:53.774191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting text to lowercase\ndf_train['keyword'] = df_train['keyword'].apply(lambda x: x.lower())\ndf_train['location'] = df_train['location'].apply(lambda x: x.lower())\ndf_train['text'] = df_train['text'].apply(lambda x: x.lower())\n\ndf_test['keyword'] = df_test['keyword'].apply(lambda x: x.lower())\ndf_test['location'] = df_test['location'].apply(lambda x: x.lower())\ndf_test['text'] = df_test['text'].apply(lambda x: x.lower())","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:53.777471Z","iopub.execute_input":"2024-08-25T17:01:53.777926Z","iopub.status.idle":"2024-08-25T17:01:53.810220Z","shell.execute_reply.started":"2024-08-25T17:01:53.777885Z","shell.execute_reply":"2024-08-25T17:01:53.808870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine the three columns into one and drop the rest\ndf_train['text'] = df_train['keyword'] + \" \" + df_train['location'] + \" \" + df_train['text']\ndf_train.drop(['id', 'keyword', 'location'], axis=1, inplace=True)\n\ndf_test['text'] = df_test['keyword'] + \" \" + df_test['location'] + \" \" + df_test['text']\ndf_test.drop(['id', 'keyword', 'location'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:53.812145Z","iopub.execute_input":"2024-08-25T17:01:53.812691Z","iopub.status.idle":"2024-08-25T17:01:53.837252Z","shell.execute_reply.started":"2024-08-25T17:01:53.812660Z","shell.execute_reply":"2024-08-25T17:01:53.835778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove hashtags, URLs, mentions i.e. @, and punctuations\ndf_train['text'] = df_train['text'].apply(lambda x: x.replace(\"#\", \"\"))\ndf_train['text'] = df_train['text'].apply(lambda x: re.sub(r'https?://[\\n\\S]+\\b', '', x))\ndf_train['text'] = df_train['text'].apply(lambda x: x.replace(\"@\", \"\"))\ndf_train['text'] = df_train['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n\ndf_test['text'] = df_test['text'].apply(lambda x: x.replace(\"#\", \"\"))\ndf_test['text'] = df_test['text'].apply(lambda x: re.sub(r'https?://[\\n\\S]+\\b', '', x))\ndf_test['text'] = df_test['text'].apply(lambda x: x.replace(\"@\", \"\"))\ndf_test['text'] = df_test['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:53.838680Z","iopub.execute_input":"2024-08-25T17:01:53.839042Z","iopub.status.idle":"2024-08-25T17:01:53.983385Z","shell.execute_reply.started":"2024-08-25T17:01:53.839013Z","shell.execute_reply":"2024-08-25T17:01:53.982040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Stop words removal (for later)","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:53.987774Z","iopub.execute_input":"2024-08-25T17:01:53.988256Z","iopub.status.idle":"2024-08-25T17:01:53.993940Z","shell.execute_reply.started":"2024-08-25T17:01:53.988215Z","shell.execute_reply":"2024-08-25T17:01:53.992417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lemmatization (for later)","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:53.996022Z","iopub.execute_input":"2024-08-25T17:01:53.996628Z","iopub.status.idle":"2024-08-25T17:01:54.013220Z","shell.execute_reply.started":"2024-08-25T17:01:53.996587Z","shell.execute_reply":"2024-08-25T17:01:54.011728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization 1 - target distribution\nfig, ax = plt.subplots()\ndf_train['target'].value_counts().plot(ax=ax, kind='bar')","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:54.015321Z","iopub.execute_input":"2024-08-25T17:01:54.015998Z","iopub.status.idle":"2024-08-25T17:01:54.331810Z","shell.execute_reply.started":"2024-08-25T17:01:54.015956Z","shell.execute_reply":"2024-08-25T17:01:54.330278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data visualization 2 - word cloud\ntext = df_train['text'].str.cat(sep=' ')\nwc = wordcloud.WordCloud().generate(text)\nplt.imshow(wc)","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:01:54.333527Z","iopub.execute_input":"2024-08-25T17:01:54.333935Z","iopub.status.idle":"2024-08-25T17:01:56.107546Z","shell.execute_reply.started":"2024-08-25T17:01:54.333899Z","shell.execute_reply":"2024-08-25T17:01:56.105751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The Model","metadata":{}},{"cell_type":"markdown","source":"(..from the starter notebook)\n\nBERT stands for Bidirectional Encoder Representations from Transformers. BERT and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models.\n\nThe BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers.\n\nBERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n\nDistilBERT model is a distilled form of the BERT model. The size of a BERT model was reduced by 40% via knowledge distillation during the pre-training phase while retaining 97% of its language understanding abilities and being 60% faster.\n\nText input needs to be transformed to embeddings before provided as input to BERT.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\n\ntorch.set_grad_enabled(False)","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:03:26.340169Z","iopub.execute_input":"2024-08-25T17:03:26.340644Z","iopub.status.idle":"2024-08-25T17:03:32.112202Z","shell.execute_reply.started":"2024-08-25T17:03:26.340608Z","shell.execute_reply":"2024-08-25T17:03:32.110798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\nMODEL_NAME = \"bert-large-uncased\"\n\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\nmodel = BertModel.from_pretrained(MODEL_NAME)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-08-25T17:03:43.797128Z","iopub.execute_input":"2024-08-25T17:03:43.797892Z","iopub.status.idle":"2024-08-25T17:05:44.687208Z","shell.execute_reply.started":"2024-08-25T17:03:43.797854Z","shell.execute_reply":"2024-08-25T17:05:44.685464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer(df_train['text'], return_tensors=\"pt\")\nwith torch.no_grad():\n    logits = model(**inputs).logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf ~/.cache/huggingface/transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-24T22:44:41.685831Z","iopub.execute_input":"2024-08-24T22:44:41.686678Z","iopub.status.idle":"2024-08-24T22:44:42.839584Z","shell.execute_reply.started":"2024-08-24T22:44:41.686636Z","shell.execute_reply":"2024-08-24T22:44:42.838030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}